Title: Omission bias

----

Text: <p>The&nbsp;<em>omission bias</em>&nbsp;refers to our tendency to judge harmful&nbsp;<em>actions</em>&nbsp;as worse than harmful&nbsp;<em>inactions</em>, even if they result in similar consequences.</p><p>Generally, most people want to do good and avoid causing harm in their everyday lives. We like to feel altruistic and compassionate. Although there is often gray area, we try to listen to our internal barometer of morality and act accordingly. Yet, sometimes the moral judgments we make are grounded in biased thinking. The omission bias causes us to view&nbsp;<em>actions&nbsp;</em>as worse than&nbsp;<em>omissions&nbsp;</em>(cases where someone fails to take action) in situations where they both have adverse consequences and similar intentions.</p><p>A 1994 study by David Asch and his colleagues explored how the omission bias affects parents’ decisions of whether to vaccinate their kids.8&nbsp;Some parents choose not to have their children vaccinated for pertussis (also known as ‘whooping cough’) because of “fears that reaction to the vaccine itself may lead to death or serious injury”. Medical data proves these fears to be negligible. In the 1970’s Britain, there was a decline in pertussis vaccinations that resulted in a major increase in cases and pertussis related deaths. Thus, the researchers used the real-life example of the pertussis vaccine to examine these decisions with historical relevance.</p><p>Asch and his team administered a questionnaire to parents about the vaccine and various questions testing their bias. Their results showed that respondents who reported they would not vaccinate their kids were “more likely to believe that vaccinating was more dangerous than not vaccinating” and were “more likely to exhibit omission bias”. Even though vaccinating had much lower probabilities of causing harm than not vaccinating, parents with the omission bias favored inaction over action.</p><p>Imagine that you are standing beside a train track. A group of five people is crossing the track. Suddenly you notice a train barrelling uncontrollably towards the group.</p><p>There is a fork in the track ahead of the group. Beside you is a lever, which you can pull to divert the train at the fork, saving the lives of the five people. However, there is a single person on the other track, who will be killed if you divert the train. Do you pull the lever to save the group of five, sacrificing one person’s life in the process?</p><p>In this scenario of a famous thought experiment first conceived in the 1970s and dubbed “the trolley problem” by Judith Thomson, around&nbsp;<u><a href="https://content.apa.org/record/2011-27114-001" target="_blank" rel="noopener noreferrer nofollow">90% of people</a></u>&nbsp;say ‘yes’. Most people justify their decision on the basis of the resulting outcome: yes, one person dies, but you have saved five people’s lives. The alternative would have been to do nothing, resulting in five people dying.</p><p>Now imagine exactly the same scenario: there’s an out of control train, and a group of five people crossing the tracks. But instead of a lever, there is a person standing right beside the track. The only way to stop the train is to push the person in front of it. This will result in this person’s death, but will stop the train from killing the group of five.</p><p>What do you do now? In this situation, most people say they would not push the person. And yet, in terms of the outcome, which was how we justified our actions in the first scenario, nothing has changed. One person would be killed to save the life of five.</p><p>As illustrated in the trolley problem, we feel much less responsible for outcomes from our omissions than our commissions.</p>

----

Tags: We notice things already primed in memory or repeated often